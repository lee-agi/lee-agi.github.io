---
{"dg-publish":true,"permalink":"/010-outbox/ilya-sutskever-we-removingfromtheageofscalingtotheageofresearch-2025-m11-summary/"}
---

# 

## 封面
- 标题：Ilya Sutskever – We're moving from the age of scaling to the age of research
- 链接：https://www.youtube.com/watch?v=aR20FWCCjAs
- 发布日期：2025-11-25
- 总字数：3369
- 预估阅读时长：约 12 分钟
- 生成时间：2025-11-25 19:07:14
- 覆盖时长：01:36:01

## 摘要总结
- Ilya 认为当前“慢起飞”的体感来自AI在evals上强，却在实际经济影响上滞后；根因可能是公司把RL训练目标对齐到evals本身，形成了“**人类版的reward hacking**”，叠加模型泛化不足。  
- 他主张把“超智能（superintelligence）”理解为“会学的心智”，而非一次性“全知全能”的成品心智；部署应走**“continual learning（持续学习）”**，并通过逐步公开让社会“感受到AI”。  
- 他提出一个非共识方向：与其构建“自我改进AI”，不如构建“稳健关怀sentient life（有感知生命）”的AI，且对最强系统的权力设“上限”。  
- 他判断**“规模时代”将转入“研究时代”，真正的瓶颈是“可靠泛化”，而非仅仅算力与数据；**“value function（值函数）”会让RL更高效，但不是本质突破**。  
- 对未来，他给出5–20年的时间窗（人类式学习者→超人）；SSI走差异化技术路径，强调泛化原理与逐步部署安全，预期最终各家会在“大战略”（协作、安全、社会适配）上趋同。

## 全文
### 现实与“慢起飞”的体感：eval很强，经济影响偏弱（00:00:00 - 00:02:23）
**Dwarkesh**：你知道最疯狂的是什么吗？  
**Ilya**：这都是真的——AI、Bay Area的热度，像是科幻在落地。  
**Dwarkesh**：还有个疯狂点是“慢起飞”有多正常；即使投到AI的资金到1% GDP，也没有想象中“天翻地覆”的感觉。  
**Ilya**：人适应得很快；而且它很抽象——新闻里只看到“某公司宣布投资X亿美元”，除此之外很难“被感受到”。但我认为AI的影响会很强地扩散进经济里。

### eval-现实落差与RL训练的“人类reward hacking”（00:02:23 - 00:05:06）
**Dwarkesh**：模型看起来比其经济影响“聪明”得多。  
**Ilya**：这是困惑点：模型在evals上表现很好，可真实世界里会在简单处重复犯错。比如你让模型修一个“vibe coding”的bug，它承认并修了却引入第二个bug；你指出后，它又把第一个bug带回来，循环往复。这暗示有些怪事。  
我有两个解释：  
- 偏“异想天开”的是RL训练让模型过于“single-minded（单念）”，缺乏广角觉察。  
- 更实在的是：pre-training时数据选择=“everything”；而RL时必须选择“训练环境”，公司往往从evals“反向取经”来设计RL环境，使得模型“看起来”在eval好，但对泛化的提升有限。叠加模型本就“泛化不足”，就形成了eval与现实表现的脱节。  
**Dwarkesh**：真正的reward hacking不是AI，而是研究者对evals的过度优化。

### 该扩展环境，还是学会“从一处学会，处处可用”？（00:05:06 - 00:06:06）
**Dwarkesh**：两条路：  
- 扩大环境覆盖面，不只优化“coding competition”，还要能产出更“tasteful”的工程成果；  
- 或者更根本地问：为什么“竞赛超人”不等于“更有品味的程序员”？那应找到能“以一通万”的学习方法。

### 人类类比：10,000小时竞赛编程 vs “it factor”（00:06:07 - 00:09:02）
**Ilya**：类比两位学生——A花1万小时刷competitive programming，掌握全部技巧；B只练100小时也能拿高分。谁职业发展更好？通常是B。今天的模型更像A：我们把所有题目、数据增强全喂一遍，变成“领域内神手”，但对其他任务未必泛化。  
**Dwarkesh**：那B的“100小时之前”是什么？  
**Ilya**：是“it factor”。  
**Dwarkesh**：这跟pre-training的区别？  
**Ilya**：pre-training强在两点：量巨大、数据“天然”（人类世界投影成文本）。但它如何支撑模型，难以解释；“人类版pre-training”的明确类比并不存在。

### 儿童期、进化与“value function（值函数）”（00:09:39 - 00:14:19）
**Dwarkesh**：人类前十几年、或“进化”是否类比pre-training？  
**Ilya**：有相似处，也有巨大不同。人类以“极少数据”学到“更深刻”的东西，并避免许多AI会犯的错。举例：有人因脑损伤丧失情绪，却变得无法做决策——说明“情绪”像是“value function”的一部分。  
**Dwarkesh**：那请你给听众定义下value function。  
**Ilya**：在当下的RL范式里，模型做长轨迹行动后得到一个分数，这个分数被传播成对每一步的训练信号。value function刻画“某状态/策略的预期回报”，但当前主流实践里它并不突出。

### 从“规模时代”回到“研究时代”的边界：真正的瓶颈是“泛化”（00:23:20 - 00:25:18）
**Ilya**：当我说“回到研究时代”，我指的不是继续盲目“scale”，而是问：你在做的事是否是对算力的最高效使用？value function能让RL更高效，但你不靠它也能做到，只是更慢。根本问题在于：这些模型的“generalization（泛化）”远逊于人类，这是显而易见且极其基础的短板。

### 样本效率、人类鲁棒性与进化先验（00:25:18 - 00:31:26）
**Dwarkesh**：人类为何样本效率高、且学习更“鲁棒”？  
**Ilya**：部分是进化先验：如locomotion、vision。儿童十几个小时就能学会开车，很可能是因为5岁时的“车类识别”就已足够强；但在语言、数学、coding上并无深厚进化先验，人类依旧学得很好，说明人类有“更根本的学习机制”。  
青少年学车能自我修正、无需显式可验证reward，靠的是内在“value function”的稳健性（除成瘾等少数例外）。这提示我们“可行的ML原理”存在，但受现实限制我不便详谈；另外，人类神经元可能“每个神经元的算力”比我们估计的大。

### RL scaling的sigmoid与信息增益直觉（含工具使用体验，广告行动号召略）（00:33:01 - 00:35:37）
**Dwarkesh**：我读了一篇RL scaling论文，学习曲线像sigmoid，不同于pre-training的power law。我用Gemini 3把自己的笔记和论文放一起，让它寻找联系：它把一次binary pass/fail的样本信息建模成熵，画出“RL与监督学习在不同pass rate下的bits增益”曲线，顿悟随之而来。又用它写了实验代码验证，发现学习率调度解释了差距。这是我第一次感觉一个模型能帮我“连出我意料外的新联系”。（广告链接略）

### 研究所需算力、SSI的资源与“straight shot（直奔超智能）”（00:35:49 - 00:47:02）
**Dwarkesh**：回到研究时代还需要巨量算力吗？  
**Ilya**：不必。AlexNet两块GPU；Transformer实验8–64块（相当于今天2块）。做最强产品需要巨大算力，但做“新范式研究”未必。SSI的30亿美元里相当部分可用于研究训练；大厂的巨额算力很多花在inference和多产品线。  
**Dwarkesh**：SSI如何赚钱？直奔超智能吗？  
**Ilya**：先专注研究，“答案会浮现”。直奔超智能有好处——不被市场日常“rat race”打断；但我也越来越看重“渐进部署”，让社会“提前感受AI”。

### AGI与pre-training两个词如何“误导”，以及“把超智能理解为‘会学的心智’”（00:47:02 - 00:52:03）
**Ilya**：两个词影响了大家的思维：AGI与pre-training。pre-training给了“多做预训=普遍变强”的错觉；但人不是AGI——人靠“continual learning”。我更认可的定义是：超智能是“能学会任何工作”的心智，而非“出生即全会”。部署像“招一个超强15岁天才”去医院、去公司，在岗位上持续学。

### 广泛部署与经济增长、监管差异与速度不确定性（00:52:03 - 00:59:05）
**Ilya**：当你有“会学且学得快”的AI，广泛部署很可能带来一段时期的“快速经济增长”。速度受多因素制约：世界庞杂、物理流程慢、各国监管不同，但“快增长”是可能场景。

### 让社会“感受到AI”、公司协作与安全观的转变（00:59:05 - 01:02:03）
**Ilya**：大家很难“感觉”未来AI的力量，因此我改变了想法：应更早、更逐步地展示系统。随着AI开始“显得强”，各家公司对安全会“显著更偏执”；你已看到OpenAI与Anthropic在safety上协作苗头——我几年前就预测会出现这样的合作。

### 对齐目标：与其“自改进AI”，不如“关怀sentient life”的AI，并为最强系统设上限（01:02:03 - 01:05:01）
**Ilya**：我认为更好的愿景是构建“稳健关怀sentient life（有感知生命）”的AI；也许这比只关怀“human life”更容易，因为AI本身将是sentient，镜像神经元式的“同理心”会涌现。还应考虑给最强超智能设“能力上限”，虽然如何实现尚不明确。  
**Dwarkesh**：如果“sentient beings”的绝大多数将是AI，人类未必能实现“控制”。  
**Ilya**：这不是完美答案，但应进入“候选解”清单，供一线公司在关键时刻采用。

### 形态、集群与范式更替：关键在“可靠泛化”（01:05:01 - 01:07:17）
**Ilya**：我倾向会有多家同时产出强系统；若“continent-sized（洲级）集群”，力量会极大，因而越需要“约束/协定”。我也认为当前范式会走一段后“pe­ter out”，真正突破在“可靠泛化”。

### 长期均衡与Neuralink++；情绪是否是“对齐成功”的范例（01:07:18 - 01:16:00）
**Ilya**：短期可以是“普遍高收入”，长期政治与制度会变化。若人人有强AI代理，人可能被动出局。我不喜欢但必须考虑的一个方案是“Neuralink++”：把人部分地变成AI，使“理解”在两者间“整块传递”，让人重新成为“系统的一部分”。  
**Dwarkesh**：或许情绪是进化对齐的成功例：脑干给出“价值/奖励”指令，皮层在现代环境中具体化“成功”的含义，但仍被价值函数牵引。  
**Ilya**：真正“神秘”的是进化如何编码“高阶社会欲望”。不像嗅觉那样可直连多巴胺，社会评价是高阶抽象、需大脑整合后才可感知——基因怎样把“该在乎这玩意儿”写进去？我想过用“脑区坐标”作为连线靶点的猜想，但被“半球切除仍可重组脑区”的事实反驳了。这仍是开放之谜。

### SSI与他者的不同、合伙人去Meta的背景（01:18:12 - 01:20:24）
**Dwarkesh**：SSI会怎么做得不一样？  
**Ilya**：我们押在“泛化原理”的不同技术路径上，已经看到进展，但还需持续研究。  
关于前联合创始人去Meta：我们当时在以320亿美元估值融资，Meta提出收购；我拒绝，但他在更接近“同意”的位置，并因此享受了显著的“近端流动性”。他是唯一去Meta的人。

### 大战略的收敛、时间表与“市场如何分配收益”（01:20:25 - 01:28:30）
**Ilya**：我预期“大战略”会收敛：更强AI出现后，大家会更清楚应做什么——沟通协作、首批系统必须是“aligned”、关怀人群/民主/有感知生命的组合。  
时间表：到“人类式学习者→超人”大概5–20年。现路线会继续带来收入，但在“人类式学习”上停滞。  
“谁吃红利”方面：历史表明新能力会被快速模仿，竞争推动“专业化”与“价格下行”。你会看到不同公司在不同复杂产业领域形成突出专长与护城河，而不是“一家通吃”。

### 多样性与self-play：为何LLMs这么像、如何激发“异见”（01:28:30 - 01:32:38）
**Dwarkesh**：复制“100万个Ilya”是否线性提效？  
**Ilya**：会有递减收益；你想要“不同想法”的人（或agent），而非“同质拷贝”。  
LLMs彼此相似，是因为pre-training数据基本同质；差异主要来自RL/post-training。至于self-play，它提供“只用算力、不靠外部数据”的有趣路，但过去形式偏窄（谈判、博弈、策略），更广泛的“对抗结构”（如debate、prover–verifier、LLM judge寻找错误）正在成为现实。关键是用“竞争”激励“差异化探索”，以生发真正的认知多样性。

### 研究品味（research taste）：从“大脑得到的正确灵感”与“美感”出发（01:32:39 - 01:35:38）
**Ilya**：我受一种审美引导：追求“美、简单、优雅、从大脑得到的正确灵感”。像“人工神经元”“分布式表征”“从经验中学习”等都符合这套美学。它支撑一种“top-down belief（自上而下的信念）”：当实验短期不顺或有bug时，你知道“事理上它应当可行”，从而坚持把路走通。

---

加粗标注的非共识/关键信息：
- **真正的“reward hacking”在研究者：把RL环境设计成对齐evals，掩盖了泛化的贫弱。**  
- **“超智能”应被定义为“会学的心智”，而非“预训到一次性全会”。**  
- **“可靠泛化”而非“更大规模”，才是下个时代的关键突破口。**  
- **优先构建“关怀sentient life”的AI，并考虑对最强系统“设能力上限”。**  
- **应更早、更渐进地“让社会感受到AI”，随能力上升安全观会自然“更偏执”。**

## 欢迎交流与合作
目前主要兴趣是探索agent的落地，想进一步交流可加微信（cleezhang），一些[自我介绍](https://lee-agi.github.io/85ed64eda0/)。

> 本文发表于 2025-11-26_周三。